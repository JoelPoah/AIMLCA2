{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA2 Clustering resign\n",
    "By : Joel Poah  \n",
    "Class :  DAAA/FT/2A/02  \n",
    "admin no : P2112729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal is to cluster those employees that will resign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "cluster = pd.read_csv('Company_Employee.csv')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA process and finding out more of data\n",
    "## Fortunately there is no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all modules used \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will split the dataframe to numeric and categorical for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns 0 ,4,5,6,8,9,10,11\n",
    "num0 = cluster.iloc[:,0]\n",
    "num1 = cluster.iloc[:,4:7]\n",
    "num2= cluster.iloc[:,8:12]\n",
    "num =pd.concat([num0,num1,num2],axis=1)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = cluster.iloc [:,1:4]\n",
    "cat1= cluster.iloc [:,7]\n",
    "cat2= cluster.iloc[:,12]\n",
    "cat=pd.concat([cat,cat1,cat2],axis=1)\n",
    "cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the numerical data\n",
    "#scale the 0th 1st and 4th column with standard scaler\n",
    "scaler = StandardScaler()\n",
    "columns_to_scale1,columns_to_scale2,columns_to_scale3 = num.iloc[:,0:2],num.iloc[:,4],num.iloc[:,7]\n",
    "columns_to_scale = pd.concat([columns_to_scale1,columns_to_scale2,columns_to_scale3],axis=1)\n",
    "columns_to_scale = scaler.fit_transform(columns_to_scale)\n",
    "columns_to_scale = pd.DataFrame(columns_to_scale,columns=['Age','Distance_Company_Home_KM','Salary','Length_of_Service_Years'])\n",
    "num_scaled=num.drop(['Age','Distance Between Company and Home (KM)','Salary ($)','Length of Service (Years)'],axis=1)\n",
    "\n",
    "# add columns_to_scale to num_scaled dataframe\n",
    "num_scaled = pd.concat([num_scaled,columns_to_scale],axis=1)\n",
    "num_scaled.rename(columns={\"Education (1 is lowest, 5 is highest)\": \"Education\",\"Job Satisfaction (1 is lowest, 4 is highest)\":\"Job_Satis\",\"Performance Rating (1 is lowest, 4 is highest)\":\"PerformRating\",\"Work Life Balance (1 is worst, 4 is best)\":\"WorkLifeBalance\"},inplace=True)\n",
    "num_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based On Dendrogram there are 3 huge clusters out of the whole numerical dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "fig = plt.gcf()\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(num,method='ward')\n",
    "\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(mergings,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    "           p = 6\n",
    ")\n",
    "\n",
    "\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality curse , there are too many dimesnions in this dataset and would need to be reduced or else model will be overfitted clusters will become less obvious as everything seems to be very close together\n",
    "### Chi Square test to find independence on categorical data.  \n",
    "One hot encode will be used for Business Travel,Department,Marital Status \n",
    "0 and 1 will be used for Resign Status and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Business travel:\",cat.BusinessTravel.unique())\n",
    "print(\"Gender :\",cat.Gender.unique())\n",
    "print(\"Department:\",cat['Job Function'].unique())\n",
    "print(\"Marital Status:\",cat.MaritalStatus.unique())\n",
    "print(\"Resign Status:\",cat['Resign Status'].unique())\n",
    "dummy = pd.get_dummies(cat)\n",
    "dummy.drop('Resign Status_No',axis=1,inplace=True)\n",
    "# rename num\n",
    "num.rename(columns={\"Education (1 is lowest, 5 is highest)\": \"Education\",\"Job Satisfaction (1 is lowest, 4 is highest)\":\"Job_Satis\",\"Performance Rating (1 is lowest, 4 is highest)\":\"PerformRating\",\"Work Life Balance (1 is worst, 4 is best)\":\"WorkLifeBalance\"},inplace=True)\n",
    "df_noscale = pd.concat([num[['Education','Job_Satis','PerformRating','WorkLifeBalance']],dummy],axis=1)\n",
    "df_noscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.concat([num,dummy],axis=1)\n",
    "# Clustering silhouette_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "# trying different number of clusters\n",
    "for k in range(2, 11):\n",
    "    model = KMeans(n_clusters=k).fit(raw)\n",
    "    label = model.labels_\n",
    "    sil_coeff = silhouette_score(raw, label, metric='euclidean')\n",
    "    print(\"For n_clusters={}, The Silhouette Coefficient is {:.3f}\".format(k,\n",
    "    sil_coeff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "X = df_noscale.drop(['Resign Status_Yes'],axis =1)\n",
    "# X= pd.concat([num,X],axis=1)\n",
    "y=df_noscale['Resign Status_Yes'] \n",
    "\n",
    "chi_scores = chi2(X,y)\n",
    "chi_scores\n",
    "# first array represents chi square values\n",
    "# second array represents p-values\n",
    "\n",
    "#null hypothesis attribute is independent\n",
    "#alternate hypothesis is attribute is not independent\n",
    "\n",
    "#element 1 and element 2 of p-values is high , it is independent and does not contribute \n",
    "#much to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### null hypothesis attribute is independent  \n",
    "alternate hypothesis is attribute is not independent\n",
    "\n",
    "#### If p-values is high , it is independent and does not contribute much to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.Series(chi_scores[1],index = X.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)\n",
    "p_values.plot.barh(figsize = (20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anova test for both categorical and numerical data\n",
    "If p-values are less than .05, this means that this factors have a statistically significant effect on resignation.\n",
    "Those marked red should be dropped since p-value is above 0.05 and is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing 2 way Anova \n",
    "df_noscale.rename(columns={\"Resign Status_Yes\": \"ResignStatus\",\"BusinessTravel_Non-Travel\":\"BusinessNonTravel\",\"BusinessTravel_Travel_Frequently\":\"BusinessFrequentlyTravel\",\"Job Function_Human Resources\":\"Job_HR\",\"Job Function_Research & Development\": \"Job_RnD\",\"Job Function_Sales\":\"Job_Sales\"},inplace=True)  #renaming the column name\n",
    "df = pd.concat([num_scaled,df_noscale],axis=1)\n",
    "model = ols('ResignStatus ~ Distance_Company_Home_KM + MaritalStatus_Married + MaritalStatus_Single + MaritalStatus_Divorced + Education + Job_Satis + PerformRating + WorkLifeBalance + Gender_Female + Gender_Male + BusinessNonTravel + BusinessFrequentlyTravel + Job_HR + Job_RnD + Job_Sales + Age + Length_of_Service_Years + Salary' ,data=df).fit()\n",
    "anova=sm.stats.anova_lm(model, typ=1).sort_values('PR(>F)',ascending=False).round(10).style.bar(subset=[\"PR(>F)\"],color ='red')\n",
    "anova.bar(subset=[\"F\"],color ='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(data, target, significance_level=0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<significance_level):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "    return best_features\n",
    "df_X = df_noscale.drop(['ResignStatus'],axis =1)\n",
    "df_y = df_noscale['ResignStatus']\n",
    "forward_selection(df_X,df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sequential Forward Selection(sfs)\n",
    "sfs = SFS(LinearRegression(),\n",
    "          k_features=11,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'r2',\n",
    "          cv = 0)\n",
    "sfs.fit(df_X, df_y)\n",
    "print(sfs.k_feature_names_)  #best features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1 = SFS(LinearRegression(),\n",
    "         k_features=(3,11),\n",
    "         forward=True,\n",
    "         floating=False,\n",
    "         cv=0)\n",
    "sfs1.fit(X, y)\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<SL_in):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "            while(len(best_features)>0):\n",
    "                best_features_with_constant = sm.add_constant(data[best_features])\n",
    "                p_values = sm.OLS(target, best_features_with_constant).fit().pvalues[1:]\n",
    "                max_p_value = p_values.max()\n",
    "                if(max_p_value >= SL_out):\n",
    "                    excluded_feature = p_values.idxmax()\n",
    "                    best_features.remove(excluded_feature)\n",
    "                else:\n",
    "                    break \n",
    "        else:\n",
    "            break\n",
    "    return best_features\n",
    "stepwise_selection(df_X,df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "X = df.drop(['ResignStatus'],axis=1)\n",
    "y= df['ResignStatus']\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "\n",
    "efs1 = EFS(knn, \n",
    "           min_features=1,\n",
    "           max_features=6,\n",
    "           scoring='accuracy',\n",
    "           print_progress=True) ### Use cross-validation generator here\n",
    "\n",
    "efs1 = efs1.fit(X, y)\n",
    "\n",
    "print('Best accuracy score: %.2f' % efs1.best_score_)\n",
    "print('Best subset (corresponding names):', efs1.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save efs1 to a txt file\n",
    "efs1.best_feature_names_\n",
    "# efs1.best_feature_names_.tolist()\n",
    "f = open(\"efs1.txt\", \"a\")\n",
    "f.write(str(efs1.best_feature_names_))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = efs1.get_metric_dict()\n",
    "\n",
    "fig = plt.figure()\n",
    "k_feat = sorted(metric_dict.keys())\n",
    "avg = [metric_dict[k]['avg_score'] for k in k_feat]\n",
    "\n",
    "upper, lower = [], []\n",
    "for k in k_feat:\n",
    "    upper.append(metric_dict[k]['avg_score'] +\n",
    "                 metric_dict[k]['std_dev'])\n",
    "    lower.append(metric_dict[k]['avg_score'] -\n",
    "                 metric_dict[k]['std_dev'])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5.5)\n",
    "plt.fill_between(k_feat,\n",
    "                 upper,\n",
    "                 lower,\n",
    "                 alpha=0.2,\n",
    "                 color='blue',\n",
    "                 lw=1)\n",
    "\n",
    "plt.plot(k_feat, avg, color='blue', marker='o')\n",
    "plt.ylabel('Accuracy +/- Standard Deviation')\n",
    "plt.xlabel('Number of Features')\n",
    "feature_min = len(metric_dict[k_feat[0]]['feature_idx'])\n",
    "feature_max = len(metric_dict[k_feat[-1]]['feature_idx'])\n",
    "plt.xticks(k_feat, \n",
    "           [str(metric_dict[k]['feature_names']) for k in k_feat], \n",
    "           rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Clustering\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "# fit a Kmeans model to the data\n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(df)\n",
    "# Visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "y_kmeans = model.predict(df)\n",
    "# scatter plot the sepal length versus sepal width\n",
    "plt.scatter(df['Distance_Company_Home_KM'], df['ResignStatus'], c=y_kmeans, s=50, cmap='viridis')\n",
    "# plot centroids for each cluster\n",
    "centers = model.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8)\n",
    "plt.xlabel('Salary ($)')\n",
    "plt.ylabel('age')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering silhouette_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "# trying different number of clusters\n",
    "for k in range(2, 11):\n",
    "    model = KMeans(n_clusters=k).fit(df)\n",
    "    label = model.labels_\n",
    "    sil_coeff = silhouette_score(X, label, metric='euclidean')\n",
    "    print(\"For n_clusters={}, The Silhouette Coefficient is {:.3f}\".format(k,\n",
    "    sil_coeff))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a00f272cf4b6b21534f4cce92f165e847380034a1edd2638bef2aea3f81e5c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
